{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import Hasoc.config as config\n",
    "import Hasoc.dataset.dataset as dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_loader(text, lbls, bs, is_test=False, drop_last=False, sampler=None, ret_dataset=False):\n",
    "    assert len(text) == len(lbls)\n",
    "    ds = dataset.BertDataset(text, lbls, is_test=is_test)\n",
    "    if ret_dataset: return ds\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=bs, drop_last=drop_last,\n",
    "                                        num_workers=5, sampler=sampler)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    #from fastai\n",
    "    y_int = True\n",
    "    def __init__(self, eps:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.eps,self.reduction = eps,reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum': loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean\n",
    "            if self.reduction=='mean':  loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)\n",
    "\n",
    "#     def activation(self, out): return F.softmax(out, dim=-1)\n",
    "#     def decodes(self, out):    return out.argmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

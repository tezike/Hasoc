{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Fitter(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def log(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def validate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertFitter(Fitter):\n",
    "    def __init__(self, model, dataloaders, optimizer, loss_func, metrics, device, log_file='training_log.txt',scheduler=None, trial=None):\n",
    "        self.model = model\n",
    "        self.train_dl, self.valid_dl = dataloaders[0], dataloaders[1]\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        if not os.path.exists(os.path.join('..', 'outputs')): os.makedirs(os.path.join('..', 'outputs'))\n",
    "        if os.path.exists(os.path.join('..', 'outputs', f'{log_file}')): \n",
    "            os.remove(os.path.join('..', 'outputs', f'{log_file}'))\n",
    "        self.log_file = os.path.join('..', 'outputs', f'{log_file}')\n",
    "        self.loss_func = loss_func\n",
    "        self.metrics = metrics\n",
    "        self.device = device\n",
    "        self.trial = trial #for optuna\n",
    "        \n",
    "    def fit(self, epochs, return_metric=False, monitor='epoch train_loss valid_loss metric time', model_path=os.path.join('..', 'weights', 'model.pth'), show_graph=True):\n",
    "        self.model_path = model_path\n",
    "        self.log(f'{time.ctime()}')\n",
    "        self.log(f'Using device: {self.device}')\n",
    "        mb = master_bar(range(1, epochs+1)) #MAJOR\n",
    "        mb.write(monitor.split(),table=True)\n",
    "        \n",
    "        model = self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "        best_metric = -np.inf\n",
    "        train_loss, valid_loss, valid_metric = 0, 0, 0\n",
    "        train_loss_list, valid_loss_list = [], []\n",
    "        \n",
    "        for i_, epoch in enumerate(mb):\n",
    "            epoch_start = timeit.default_timer()\n",
    "            start = time.time()\n",
    "            self.log('-'*50)\n",
    "            self.log(f'Running Epoch #{epoch} {\"ðŸ”¥\"*epoch}')\n",
    "            self.log(f'{\"-\"*50} \\n')\n",
    "            self.log('TRAINING...')\n",
    "            for ind, batch in enumerate(progress_bar(self.train_dl, parent=mb)):\n",
    "                train_loss += self.train(batch, model, optimizer, self.device, self.scheduler)\n",
    "                if ind % 500 == 0:\n",
    "                    self.log(f'Batch: {ind}, Train loss: {train_loss/ len(self.train_dl)}')\n",
    "#                 break\n",
    "                mb.child.comment = f'{train_loss / (ind+1 * self.train_dl.batch_size):.3f}'\n",
    "            train_loss /= mb.child.total\n",
    "            train_loss_list.append(train_loss) #for graph\n",
    "            self.log(f'Training time: {round(time.time()-start, 2)} secs \\n')\n",
    "            \n",
    "            start = time.time()\n",
    "            self.log('EVALUATING...')\n",
    "            with torch.no_grad():\n",
    "                for ind, batch in enumerate(progress_bar(self.valid_dl, parent=mb)):\n",
    "                    valid_loss_, valid_metric_ = self.validate(batch, model, self.device)\n",
    "                    valid_loss += valid_loss_\n",
    "                    valid_metric += valid_metric_\n",
    "                    if ind % 500 == 0:\n",
    "                        self.log(f'Batch: {ind}, Valid loss: {valid_loss/ len(self.valid_dl)}')\n",
    "#                     break   \n",
    "                    mb.child.comment = f'{valid_loss / (ind+1 * self.train_dl.batch_size):.3f}'\n",
    "                \n",
    "                valid_loss /= mb.child.total\n",
    "                valid_metric /= mb.child.total\n",
    "                valid_loss_list.append(valid_loss) #for graph\n",
    "            \n",
    "            if valid_metric > best_metric:\n",
    "                #             save model\n",
    "                if self.model_path is not None:\n",
    "                    if not os.path.exists(os.path.join('..', 'weights')): os.makedirs(os.path.join('..', 'weights'))\n",
    "                    self.log(f'Saving model weights at {self.model_path}')\n",
    "                    torch.save(model.state_dict(), self.model_path)\n",
    "                best_metric = valid_metric\n",
    "                    \n",
    "            if self.trial is not None:\n",
    "                self.trial.report(best_metric, epoch)\n",
    "\n",
    "                # Handle pruning based on the intermediate value.\n",
    "                if self.trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            if show_graph:\n",
    "                self.plot_loss_update(epoch, epochs, mb, train_loss_list, valid_loss_list) # for graph\n",
    "                               \n",
    "            epoch_end = timeit.default_timer()\n",
    "            total_time = epoch_end - epoch_start\n",
    "            mins, secs = divmod(total_time, 60)\n",
    "            hours, mins = divmod(mins, 60)\n",
    "            ret_time = f'{int(hours)}:{int(mins)}:{int(secs)}'\n",
    "            mb.write([epoch,f'{train_loss:.6f}',f'{valid_loss:.6f}',f'{valid_metric:.6f}', f'{ret_time}'],table=True)\n",
    "            self.log(f'Evaluation time: {ret_time}\\n')\n",
    "#             break\n",
    "            \n",
    "        if return_metric: return best_metric\n",
    "    \n",
    "    def train(self, xy, model, opt, device, sched=None):\n",
    "        model.train()\n",
    "        y = xy.pop('targets')\n",
    "        x = xy\n",
    "        inputs, targets = [x_.to(device) for x_ in x.values()], y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(*inputs)\n",
    "        loss = self.loss_func(out, targets.argmax(dim=-1))\n",
    "        loss.backward()\n",
    "        opt.step()       \n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self, xy, model, device):\n",
    "        model.eval()\n",
    "        y = xy.pop('targets')\n",
    "        x = xy\n",
    "        inputs, targets = [x_.to(device) for x_ in x.values()], y.to(device)\n",
    "        out = model(*inputs)\n",
    "        loss = self.loss_func(out, targets.argmax(dim=-1))\n",
    "        \n",
    "#         calc f1-score\n",
    "#         magic done to make sure any number of columns can properly be calculated\n",
    "        metrics = self.metrics(targets.cpu().argmax(dim=-1), out.cpu().argmax(dim=-1))  #sklearn metrics are (targ, inp)\n",
    "        return loss.item(), metrics\n",
    "            \n",
    "    def log(self, message, verbose=False):\n",
    "        if verbose: print(message)\n",
    "        with open(self.log_file, 'a+') as logger_:\n",
    "            logger_.write(f'{message}\\n')\n",
    "            \n",
    "#     @staticmethod\n",
    "#     def calc_loss(out, targ):\n",
    "#         return nn.BCEWithLogitsLoss()(out, targ)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "            expects epoch to start from 1.\n",
    "        \"\"\"\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(np.array(graphs), np.array(x_bounds), np.array(y_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_preds(test_ds, test_dl, model,device=None, ensemble_proba=False):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    preds = np.zeros(len(test_ds))\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar(test_dl):\n",
    "            out = model(**batch)\n",
    "            if not ensemble_proba:\n",
    "                out = out.softmax(dim=-1).argmax(dim=-1)\n",
    "            else:\n",
    "                out = out.softmax(dim=-1)\n",
    "            test_preds.append(out.cpu().numpy())\n",
    "    return np.concatenate(test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

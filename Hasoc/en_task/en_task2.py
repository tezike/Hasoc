# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/en_task.en_task2.ipynb (unless otherwise specified).

__all__ = ['SEED', 'df', 'le', 'train_ds', 'train_dl', 'valid_dl', 'modeller', 'model_params', 'no_decay',
           'optimizer_params', 'lr', 'optimizer', 'num_train_steps', 'scheduler', 'fit', 'NUM_EPOCHS', 'test_df',
           'test_dl', 'modeller', 'preds', 'sub', 'submission_en_task2_df']

# Cell
import os

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import transformers

import Hasoc.config as config
import Hasoc.utils.utils as utils
import Hasoc.utils.engine as engine
import Hasoc.model.model as model
import Hasoc.dataset.dataset as dataset

from functools import partial
from sklearn.metrics import f1_score
from sklearn.preprocessing import LabelEncoder
from catalyst.data.sampler import BalanceClassSampler
from transformers import AdamW, get_linear_schedule_with_warmup

# Cell
SEED = 42
utils.seed_everything(SEED)

# Cell
df = pd.read_csv(config.DATA_PATH/'fold_df.csv')

# Cell
le = LabelEncoder()
le.fit_transform(df.task2)
le.classes_

# Cell
df['task2_encoded'] = le.transform(df.task2.values)

# Cell
train_ds = utils.create_loader(train_df.text.values, train_df.task2_encoded, bs=config.TRAIN_BATCH_SIZE,
                               ret_dataset=True)
train_dl = utils.create_loader(train_df.text.values, train_df.task2_encoded, bs=config.TRAIN_BATCH_SIZE,
                               sampler=BalanceClassSampler(labels=train_ds.get_labels(), mode="upsampling"))
valid_dl = utils.create_loader(valid_df.text.values, valid_df.task2_encoded, bs=config.VALID_BATCH_SIZE)

# Cell
modeller = model.HasocModel(len(le.classes_), drop=0.6)

# Cell
model_params = list(modeller.named_parameters())

# Cell
# we don't want weight decay for these
no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']

optimizer_params = [
    {'params': [p for n, p in model_params if n not in no_decay],
    'weight_decay':0.001},
    #  no weight decay should be applied
    {'params': [p for n, p in model_params if n in no_decay],
    'weight_decay':0.0}
]

# Cell
# lr = config.LR
lr = 1e-4

# Cell
optimizer = AdamW(optimizer_params, lr=lr)

# Cell
num_train_steps = int(len(df) / config.TRAIN_BATCH_SIZE * config.NUM_EPOCHS)

# Cell
scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,
                                                num_warmup_steps=20,
                                                num_training_steps=num_train_steps-20)

# Cell
# fit = engine.BertFitter(modeller, (train_dl, valid_dl), optimizer, nn.CrossEntropyLoss(), partial(f1_score, average='macro'), config.DEVICE, scheduler=scheduler, log_file='en_task2_log.txt')
fit = engine.BertFitter(modeller, (train_dl, valid_dl), optimizer, utils.LabelSmoothingCrossEntropy(), partial(f1_score, average='macro'), config.DEVICE, scheduler=scheduler, log_file='en_task2_log.txt')

# Cell
NUM_EPOCHS = 6
fit.fit(NUM_EPOCHS, model_path=os.path.join(config.MODEL_PATH/'en_task2.pth'))

# Cell
test_df = pd.read_csv(config.DATA_PATH/'en_task_a/english_test.csv')

# Cell
test_dl = utils.create_loader(test_df.text.values, lbls=[None]*len(test_df.text.values), bs=config.VALID_BATCH_SIZE, is_test=True)

# Cell
modeller = model.HasocModel(len(le.classes_))
modeller.load_state_dict(torch.load(config.MODEL_PATH/'en_task2.pth'))

# Cell
preds = engine.get_preds(test_dl.dataset, test_dl, modeller, config.DEVICE)

# Cell
sub = pd.read_csv(config.DATA_PATH/'en_task_a/english_test.csv')

# Cell
submission_en_task2_df = test_df.drop(columns=['text', 'task1', 'task2']).copy()

# Cell
submission_en_task2_df['task2'] = preds

# Cell
submission_en_task2_df.to_csv(os.path.join('..', 'outputs', 'submission_EN_B.csv'), index=False)
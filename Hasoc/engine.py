# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/engine.ipynb (unless otherwise specified).

__all__ = ['Fitter', 'BertFitter']

# Cell
import os
import timeit
import torch
import numpy as np

from abc import ABC
from sklearn.metrics import f1_score
from fastprogress.fastprogress import master_bar, progress_bar

# Cell
class Fitter(ABC):
    def __init__(self):
        pass

    def fit(self):
        pass

    def log(self):
        pass

    def train(self):
        pass

    def validate(self):
        pass

# Cell
class BertFitter(Fitter):
    def __init__(self, model, train_dl, valid_dl, optimizer, device, log_file='training_log.txt',scheduler=None, trial=None):
        self.model = model
        self.train_dl = train_dl
        self.valid_dl = valid_dl
        self.optimizer = optimizer
        self.scheduler = scheduler
        if not os.path.exists(os.path.join('..', 'outputs')): os.makedirs(os.path.join('..', 'outputs'))
        if os.path.exists(os.path.join('..', 'outputs', f'{log_file}')):
            os.remove(os.path.join('..', 'outputs', f'{log_file}'))
        self.log_file = os.path.join('..', 'outputs', f'{log_file}')
        self.device = device
        self.trial = trial #for optuna

    def fit(self, epochs, return_metric=False, monitor='epoch train_loss valid_loss log_metric time', model_path=os.path.join('..', 'weights', 'model.pth'), show_graph=True):
        self.model_path = model_path
        self.log(f'{time.ctime()}')
        self.log(f'Using device: {self.device}')
        mb = master_bar(range(1, epochs+1)) #MAJOR
        mb.write(monitor.split(),table=True)

        model = self.model.to(self.device)
        optimizer = self.optimizer
        best_metric = -np.inf
        train_loss, valid_loss, valid_metric = 0, 0, 0
        train_loss_list, valid_loss_list = [], []

        for i_, epoch in enumerate(mb):
            epoch_start = timeit.default_timer()
            start = time.time()
            self.log('-'*50)
            self.log(f'Running Epoch #{epoch} {"ðŸ”¥"*epoch}')
            self.log(f'{"-"*50} \n')
            self.log('TRAINING...')
            for ind, batch in enumerate(progress_bar(self.train_dl, parent=mb)):
                train_loss += self.train(batch['x'], batch['y'], model, optimizer, self.device, self.scheduler)
                if ind % 500 == 0:
                    self.log(f'Batch: {ind}, Train loss: {train_loss/ len(self.train_dl)}')
                if ind == 50: break
                mb.child.comment = f'{train_loss / (ind+1 * self.train_dl.batch_size):.3f}'
            train_loss /= mb.child.total
            train_loss_list.append(train_loss) #for graph
            self.log(f'Training time: {round(time.time()-start, 2)} secs \n')

            start = time.time()
            self.log('EVALUATING...')
            with torch.no_grad():
                for ind, batch in enumerate(progress_bar(self.valid_dl, parent=mb)):
                    valid_loss_, valid_metric_ = self.validate(batch['x'], batch['y'], model, self.device)
                    valid_loss += valid_loss_
                    valid_metric += valid_metric_
                    if ind % 500 == 0:
                        self.log(f'Batch: {ind}, Valid loss: {valid_loss/ len(self.valid_dl)}')
                    if ind == 50: break
                    mb.child.comment = f'{valid_loss / (ind+1 * self.train_dl.batch_size):.3f}'

                valid_loss /= mb.child.total
                valid_metric /= mb.child.total
                valid_loss_list.append(valid_loss) #for graph

            if valid_matric > best_metric:
                #             save model
                if self.model_path is not None:
                    if not os.path.exists(os.path.join('..', 'weights')): os.makedirs(os.path.join('..', 'weights'))
                    self.log(f'Saving model weights at {self.model_path}')
                    torch.save(model.state_dict(), self.model_path)
                best_metric = valid_metric

            if self.trial is not None:
                self.trial.report(best_metric, epoch)

                # Handle pruning based on the intermediate value.
                if self.trial.should_prune():
                    raise optuna.exceptions.TrialPruned()

            if show_graph:
                self.plot_loss_update(epoch, epochs, mb, train_loss_list, valid_loss_list) # for graph

            epoch_end = timeit.default_timer()
            total_time = epoch_end - epoch_start
            mins, secs = divmod(total_time, 60)
            hours, mins = divmod(mins, 60)
            ret_time = f'{int(hours)}:{int(mins)}:{int(secs)}'
            mb.write([epoch,f'{train_loss:.6f}',f'{valid_loss:.6f}',f'{valid_loss:.6f}', f'{ret_time}'],table=True)
            self.log(f'Evaluation time: {ret_time}\n')
            if i_ == 2: break

        if return_metric: return best_metric

    def train(self, x, y, model, opt, device, sched=None):
        model.train()
        inputs, targets = x.to(device), y.to(device)
        opt.zero_grad()
        out = model(inputs)
        loss = self.calc_loss(out, targets)
        loss.backward()
        opt.step()
        if sched is not None:
            sched.step()
        return loss.item()

    def validate(self, x, y, model, device):
        model.eval()

        inputs, targets = x.to(device), y.to(device)
        out = model(inputs)
        loss = self.calc_loss(out, targets)

#         calc f1-score
        f1score = f1_score(y, out)
        return loss.item(), score


    def log(self, message, verbose=False):
        if verbose: print(message)
        with open(self.log_file, 'a+') as logger_:
            logger_.write(f'{message}\n')

    @staticmethod
    def calc_loss(out, targ):
        return nn.BCEWithLogitsLoss()(out, targ)

    @staticmethod
    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):
        """ dynamically print the loss plot during the training/validation loop.
            expects epoch to start from 1.
        """
        x = range(1, epoch+1)
        y = np.concatenate((train_loss, valid_loss))
        graphs = [[x,train_loss], [x,valid_loss]]
        x_margin = 0.2
        y_margin = 0.05
        x_bounds = [1-x_margin, epochs+x_margin]
        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]

        mb.update_graph(np.array(graphs), np.array(x_bounds), np.array(y_bounds))
# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/main.ipynb (unless otherwise specified).

__all__ = ['MODEL_NAME', 'MAX_SEQ_LEN', 'device', 'df', 'bert_tokenizer', 'pad', 'custom_tokenizer', 'tfms', 'splits',
           'dsets', 'dls_clas', 'model', 'learn']

# Cell
MODEL_NAME = 'bert-base-uncased'
MAX_SEQ_LEN = 72
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# Cell
df = pd.read_csv(os.path.join('../data', 'en_task_a', 'hasoc_2020_en_train_new.csv'), sep='\t')

# Cell
df.tweet_id = df.tweet_id.astype('int').astype('str')

# Cell
bert_tokenizer = transformers.BertTokenizer.from_pretrained(
            pretrained_model_name_or_path=MODEL_NAME,
            do_lower_case=True,
            )

# Cell
pad = partial(pad_input_chunk, pad_first=False, pad_idx=bert_tokenizer.pad_token_id, seq_len=MAX_SEQ_LEN)

# Cell
custom_tokenizer = Tokenizer.from_df(text_cols='text', tok=utils.HFTokenizer(tokenizer=bert_tokenizer), rules=[])

# Cell
tfms = [attrgetter('text'), custom_tokenizer, Numericalize(vocab=bert2fastai_vocab), utils.Add_Special_Cls(tokenizer=bert_tokenizer)]

# Cell
splits = RandomSplitter()(df)

# Cell
dsets = Datasets(df, tfms=[tfms, [attrgetter('task1'), Categorize()]], splits=splits, dl_type=SortedDL)

# Cell
dls_clas = dsets.dataloaders(bs=56, val_bs=144, before_batch=[pad], seq_len=MAX_SEQ_LEN)

# Cell
dls_clas.show_batch(max_n=2)

# Cell
dsets.c

# Cell
model = model.HasocModel(out_feat=2)

# Cell
learn = Learner(dls_clas, model.to(device), metrics=[accuracy, F1Score(average='macro')], drop_mult=0.3, device=device, loss_func=CrossEntropyLossFlat())

# Cell
learn.lr_find()
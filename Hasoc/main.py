# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/main.ipynb (unless otherwise specified).

__all__ = ['SEED', 'MODEL_NAME', 'MAX_SEQ_LEN', 'device', 'df', 'df', 'y', 'kf', 'bert_tokenizer', 'pad',
           'custom_tokenizer', 'tfms', 'splits', 'dsets', 'dls_clas', 'modeller', 'learn', 'lr', 'dsets', 'dls_clas',
           'modeller', 'learn', 'lr']

# Cell
import os
import pandas as pd
import numpy as np
import transformers

from fastai.text.all import *
from sklearn.model_selection import StratifiedKFold

from Hasoc import utils
from Hasoc import model

# Cell
SEED = 42
utils.seed_everything(seed=SEED)

# Cell
MODEL_NAME = 'bert-base-uncased'
MAX_SEQ_LEN = 72
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# Cell
df = pd.read_csv(os.path.join('../data', 'en_task_a', 'hasoc_2020_en_train_new.csv'), sep='\t')

# Cell
df['kfold_task1'] = -1
df['kfold_task2'] = -1
df = train_df.sample(frac=1.,random_state=SEED).reset_index(drop=True)
y = df['task1'].values
kf = StratifiedKFold(n_splits=5, random_state=SEED)
for fold,(t_,v_) in enumerate(kf.split(X=df,y=y)):
    df.loc[v_,'kfold_task1'] = fold

for fold,(t_,v_) in enumerate(kf.split(X=df,y=y)):
    df.loc[v_,'kfold_task2'] = fold

# Cell
df.tweet_id = df.tweet_id.astype('int').astype('str')

# Cell
bert_tokenizer = transformers.BertTokenizer.from_pretrained(
            pretrained_model_name_or_path=MODEL_NAME,
            do_lower_case=True,
            )

# Cell
pad = partial(pad_input_chunk, pad_first=False, pad_idx=bert_tokenizer.pad_token_id, seq_len=MAX_SEQ_LEN)

# Cell
custom_tokenizer = Tokenizer.from_df(text_cols='text', tok=utils.HFTokenizer(tokenizer=bert_tokenizer), rules=[])

# Cell
tfms = [attrgetter('text'), custom_tokenizer, Numericalize(vocab=bert2fastai_vocab), utils.Add_Special_Cls(tokenizer=bert_tokenizer)]

# Cell
splits = RandomSplitter()(df)

# Cell
dsets = Datasets(df, tfms=[tfms, [attrgetter('task1'), Categorize()]], splits=splits, dl_type=SortedDL)

# Cell
dls_clas = dsets.dataloaders(bs=56, val_bs=144, before_batch=[pad], seq_len=MAX_SEQ_LEN)

# Cell
dls_clas.show_batch(max_n=2)

# Cell
modeller = model.HasocModel(out_feat=dsets.c)

# Cell
learn = Learner(dls_clas, modeller.to(device), metrics=[accuracy, F1Score(average='macro')], drop_mult=0.3, device=device, loss_func=CrossEntropyLossFlat())

# Cell
learn.lr_find()

# Cell
lr = 4e-4

learn.fit_one_cycle(7, lr, moms=(0.8,0.7,0.8), wd=0.1)

# Cell
learn.export(os.path.join('models', 'task1.pkl'))

# Cell
dsets = Datasets(df, tfms=[tfms, [attrgetter('task2'), Categorize()]], splits=splits, dl_type=SortedDL)

# Cell
dls_clas = dsets.dataloaders(bs=56, val_bs=144, before_batch=[pad], seq_len=MAX_SEQ_LEN)

# Cell
dls_clas.show_batch(max_n=2)

# Cell
modeller = model.HasocModel(out_feat=dsets.c)

# Cell
learn = Learner(dls_clas, modeller.to(device), metrics=[accuracy, F1Score(average='macro')], drop_mult=0.3, device=device, loss_func=CrossEntropyLossFlat())

# Cell
learn.lr_find()

# Cell
lr = 4e-4

learn.fit_one_cycle(7, lr, moms=(0.8,0.7,0.8), wd=0.1)

# Cell
learn.export(os.path.join('models', 'task2.pkl'))